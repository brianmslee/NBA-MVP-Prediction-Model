{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/datasets/danchyy/nba-mvp-votings-through-history?select=mvp_votings.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "mvp_voting_df = pd.read_csv('./mvp_votings.csv')\n",
    "test_data_df = pd.read_csv('./test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fga</th>\n",
       "      <th>fg3a</th>\n",
       "      <th>fta</th>\n",
       "      <th>per</th>\n",
       "      <th>ts_pct</th>\n",
       "      <th>usg_pct</th>\n",
       "      <th>bpm</th>\n",
       "      <th>season</th>\n",
       "      <th>player</th>\n",
       "      <th>win_pct</th>\n",
       "      <th>votes_first</th>\n",
       "      <th>points_won</th>\n",
       "      <th>points_max</th>\n",
       "      <th>award_share</th>\n",
       "      <th>g</th>\n",
       "      <th>mp_per_g</th>\n",
       "      <th>pts_per_g</th>\n",
       "      <th>trb_per_g</th>\n",
       "      <th>ast_per_g</th>\n",
       "      <th>stl_per_g</th>\n",
       "      <th>blk_per_g</th>\n",
       "      <th>fg_pct</th>\n",
       "      <th>fg3_pct</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>ws</th>\n",
       "      <th>ws_per_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.572</td>\n",
       "      <td>28.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1980-81</td>\n",
       "      <td>Julius Erving</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>28.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>0.658</td>\n",
       "      <td>82.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.787</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.528</td>\n",
       "      <td>24.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1980-81</td>\n",
       "      <td>Larry Bird</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>20.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>0.613</td>\n",
       "      <td>82.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>21.2</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.863</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.616</td>\n",
       "      <td>26.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1980-81</td>\n",
       "      <td>Kareem Abdul-Jabbar</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>8.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>0.414</td>\n",
       "      <td>80.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>10.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.766</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>19.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.585</td>\n",
       "      <td>27.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1980-81</td>\n",
       "      <td>Moses Malone</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>8.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>27.8</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.757</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>21.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.555</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1980-81</td>\n",
       "      <td>George Gervin</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.7</td>\n",
       "      <td>27.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.826</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   fga  fg3a   fta   per  ts_pct  usg_pct  bpm   season  \\\n",
       "0           0  18.6   0.2   6.5  25.1   0.572     28.4  8.0  1980-81   \n",
       "1           1  18.3   0.9   4.0  19.9   0.528     24.3  5.1  1980-81   \n",
       "2           2  18.2   0.0   6.9  25.5   0.616     26.3  5.3  1980-81   \n",
       "3           3  19.3   0.0  10.1  25.1   0.585     27.6  3.7  1980-81   \n",
       "4           4  21.1   0.4   7.6  22.9   0.555     32.3  1.6  1980-81   \n",
       "\n",
       "                player   win_pct  votes_first  points_won  points_max  \\\n",
       "0        Julius Erving  0.756098         28.0       454.0       690.0   \n",
       "1           Larry Bird  0.756098         20.0       423.0       690.0   \n",
       "2  Kareem Abdul-Jabbar  0.658537          8.0       286.0       690.0   \n",
       "3         Moses Malone  0.487805          8.0       180.0       690.0   \n",
       "4        George Gervin  0.634146          1.0        83.0       690.0   \n",
       "\n",
       "   award_share     g  mp_per_g  pts_per_g  trb_per_g  ast_per_g  stl_per_g  \\\n",
       "0        0.658  82.0      35.0       24.6        8.0        4.4        2.1   \n",
       "1        0.613  82.0      39.5       21.2       10.9        5.5        2.0   \n",
       "2        0.414  80.0      37.2       26.2       10.3        3.4        0.7   \n",
       "3        0.261  80.0      40.6       27.8       14.8        1.8        1.0   \n",
       "4        0.120  82.0      33.7       27.1        5.1        3.2        1.1   \n",
       "\n",
       "   blk_per_g  fg_pct  fg3_pct  ft_pct    ws  ws_per_48  \n",
       "0        1.8   0.521    0.222   0.787  13.8      0.231  \n",
       "1        0.8   0.478    0.270   0.863  10.8      0.160  \n",
       "2        2.9   0.574    0.000   0.766  14.3      0.230  \n",
       "3        1.9   0.522    0.333   0.757  13.7      0.202  \n",
       "4        0.7   0.492    0.257   0.826  10.5      0.182  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvp_voting_df.head()\n",
    "#test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network model with 11 features \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = mvp_voting_df[['pts_per_g', 'trb_per_g', 'ast_per_g', 'stl_per_g', 'blk_per_g', 'per', 'bpm', 'usg_pct', 'win_pct', 'ws', 'ws_per_48']]\n",
    "y = mvp_voting_df['award_share']\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=11, activation='relu', input_dim = 11))\n",
    "model.add(tf.keras.layers.Dense(units=22, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 11)                132       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 22)                264       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 23        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 419 (1.64 KB)\n",
      "Trainable params: 419 (1.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer = 'adam', metrics = ['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 4s 56ms/step - loss: 0.5058 - mae: 0.4912 - mse: 0.5058 - val_loss: 0.2754 - val_mae: 0.3750 - val_mse: 0.2754\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.2206 - mae: 0.3326 - mse: 0.2206 - val_loss: 0.1466 - val_mae: 0.2720 - val_mse: 0.1466\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1343 - mae: 0.2621 - mse: 0.1343 - val_loss: 0.0965 - val_mae: 0.2247 - val_mse: 0.0965\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0950 - mae: 0.2211 - mse: 0.0950 - val_loss: 0.0757 - val_mae: 0.1974 - val_mse: 0.0757\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0748 - mae: 0.1960 - mse: 0.0748 - val_loss: 0.0642 - val_mae: 0.1831 - val_mse: 0.0642\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0618 - mae: 0.1772 - mse: 0.0618 - val_loss: 0.0574 - val_mae: 0.1734 - val_mse: 0.0574\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0531 - mae: 0.1642 - mse: 0.0531 - val_loss: 0.0523 - val_mae: 0.1656 - val_mse: 0.0523\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0467 - mae: 0.1535 - mse: 0.0467 - val_loss: 0.0489 - val_mae: 0.1582 - val_mse: 0.0489\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0421 - mae: 0.1449 - mse: 0.0421 - val_loss: 0.0465 - val_mae: 0.1537 - val_mse: 0.0465\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0389 - mae: 0.1401 - mse: 0.0389 - val_loss: 0.0447 - val_mae: 0.1503 - val_mse: 0.0447\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0363 - mae: 0.1343 - mse: 0.0363 - val_loss: 0.0433 - val_mae: 0.1473 - val_mse: 0.0433\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0345 - mae: 0.1299 - mse: 0.0345 - val_loss: 0.0425 - val_mae: 0.1457 - val_mse: 0.0425\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0332 - mae: 0.1273 - mse: 0.0332 - val_loss: 0.0414 - val_mae: 0.1446 - val_mse: 0.0414\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0321 - mae: 0.1249 - mse: 0.0321 - val_loss: 0.0405 - val_mae: 0.1424 - val_mse: 0.0405\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0315 - mae: 0.1223 - mse: 0.0315 - val_loss: 0.0404 - val_mae: 0.1424 - val_mse: 0.0404\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0303 - mae: 0.1202 - mse: 0.0303 - val_loss: 0.0393 - val_mae: 0.1399 - val_mse: 0.0393\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0297 - mae: 0.1183 - mse: 0.0297 - val_loss: 0.0391 - val_mae: 0.1394 - val_mse: 0.0391\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0290 - mae: 0.1166 - mse: 0.0290 - val_loss: 0.0386 - val_mae: 0.1387 - val_mse: 0.0386\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0285 - mae: 0.1157 - mse: 0.0285 - val_loss: 0.0382 - val_mae: 0.1378 - val_mse: 0.0382\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0280 - mae: 0.1136 - mse: 0.0280 - val_loss: 0.0380 - val_mae: 0.1375 - val_mse: 0.0380\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0277 - mae: 0.1128 - mse: 0.0277 - val_loss: 0.0373 - val_mae: 0.1359 - val_mse: 0.0373\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0272 - mae: 0.1113 - mse: 0.0272 - val_loss: 0.0370 - val_mae: 0.1352 - val_mse: 0.0370\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0269 - mae: 0.1105 - mse: 0.0269 - val_loss: 0.0371 - val_mae: 0.1354 - val_mse: 0.0371\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0265 - mae: 0.1091 - mse: 0.0265 - val_loss: 0.0368 - val_mae: 0.1349 - val_mse: 0.0368\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0261 - mae: 0.1086 - mse: 0.0261 - val_loss: 0.0362 - val_mae: 0.1340 - val_mse: 0.0362\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0259 - mae: 0.1080 - mse: 0.0259 - val_loss: 0.0361 - val_mae: 0.1335 - val_mse: 0.0361\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0256 - mae: 0.1073 - mse: 0.0256 - val_loss: 0.0357 - val_mae: 0.1329 - val_mse: 0.0357\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0255 - mae: 0.1067 - mse: 0.0255 - val_loss: 0.0360 - val_mae: 0.1338 - val_mse: 0.0360\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0250 - mae: 0.1060 - mse: 0.0250 - val_loss: 0.0350 - val_mae: 0.1315 - val_mse: 0.0350\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0252 - mae: 0.1056 - mse: 0.0252 - val_loss: 0.0357 - val_mae: 0.1329 - val_mse: 0.0357\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0246 - mae: 0.1045 - mse: 0.0246 - val_loss: 0.0348 - val_mae: 0.1308 - val_mse: 0.0348\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0244 - mae: 0.1036 - mse: 0.0244 - val_loss: 0.0348 - val_mae: 0.1313 - val_mse: 0.0348\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0240 - mae: 0.1029 - mse: 0.0240 - val_loss: 0.0348 - val_mae: 0.1309 - val_mse: 0.0348\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0239 - mae: 0.1024 - mse: 0.0239 - val_loss: 0.0345 - val_mae: 0.1302 - val_mse: 0.0345\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0237 - mae: 0.1017 - mse: 0.0237 - val_loss: 0.0345 - val_mae: 0.1303 - val_mse: 0.0345\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0235 - mae: 0.1009 - mse: 0.0235 - val_loss: 0.0346 - val_mae: 0.1304 - val_mse: 0.0346\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0233 - mae: 0.1009 - mse: 0.0233 - val_loss: 0.0342 - val_mae: 0.1296 - val_mse: 0.0342\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0230 - mae: 0.1002 - mse: 0.0230 - val_loss: 0.0342 - val_mae: 0.1297 - val_mse: 0.0342\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0230 - mae: 0.0991 - mse: 0.0230 - val_loss: 0.0344 - val_mae: 0.1295 - val_mse: 0.0344\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0228 - mae: 0.0994 - mse: 0.0228 - val_loss: 0.0338 - val_mae: 0.1292 - val_mse: 0.0338\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0225 - mae: 0.0989 - mse: 0.0225 - val_loss: 0.0339 - val_mae: 0.1286 - val_mse: 0.0339\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0224 - mae: 0.0976 - mse: 0.0224 - val_loss: 0.0336 - val_mae: 0.1284 - val_mse: 0.0336\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0223 - mae: 0.0980 - mse: 0.0223 - val_loss: 0.0333 - val_mae: 0.1276 - val_mse: 0.0333\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0218 - mae: 0.0966 - mse: 0.0218 - val_loss: 0.0338 - val_mae: 0.1287 - val_mse: 0.0338\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0217 - mae: 0.0963 - mse: 0.0217 - val_loss: 0.0332 - val_mae: 0.1277 - val_mse: 0.0332\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0216 - mae: 0.0957 - mse: 0.0216 - val_loss: 0.0328 - val_mae: 0.1271 - val_mse: 0.0328\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0214 - mae: 0.0953 - mse: 0.0214 - val_loss: 0.0327 - val_mae: 0.1266 - val_mse: 0.0327\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0213 - mae: 0.0952 - mse: 0.0213 - val_loss: 0.0327 - val_mae: 0.1266 - val_mse: 0.0327\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0212 - mae: 0.0952 - mse: 0.0212 - val_loss: 0.0325 - val_mae: 0.1267 - val_mse: 0.0325\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0210 - mae: 0.0941 - mse: 0.0210 - val_loss: 0.0329 - val_mae: 0.1273 - val_mse: 0.0329\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0207 - mae: 0.0939 - mse: 0.0207 - val_loss: 0.0322 - val_mae: 0.1259 - val_mse: 0.0322\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0206 - mae: 0.0937 - mse: 0.0206 - val_loss: 0.0325 - val_mae: 0.1267 - val_mse: 0.0325\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0205 - mae: 0.0933 - mse: 0.0205 - val_loss: 0.0323 - val_mae: 0.1261 - val_mse: 0.0323\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0204 - mae: 0.0929 - mse: 0.0204 - val_loss: 0.0319 - val_mae: 0.1256 - val_mse: 0.0319\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0203 - mae: 0.0920 - mse: 0.0203 - val_loss: 0.0321 - val_mae: 0.1261 - val_mse: 0.0321\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0202 - mae: 0.0924 - mse: 0.0202 - val_loss: 0.0317 - val_mae: 0.1248 - val_mse: 0.0317\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0201 - mae: 0.0914 - mse: 0.0201 - val_loss: 0.0328 - val_mae: 0.1278 - val_mse: 0.0328\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0200 - mae: 0.0920 - mse: 0.0200 - val_loss: 0.0318 - val_mae: 0.1257 - val_mse: 0.0318\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0197 - mae: 0.0908 - mse: 0.0197 - val_loss: 0.0316 - val_mae: 0.1251 - val_mse: 0.0316\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0195 - mae: 0.0898 - mse: 0.0195 - val_loss: 0.0317 - val_mae: 0.1255 - val_mse: 0.0317\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0193 - mae: 0.0898 - mse: 0.0193 - val_loss: 0.0315 - val_mae: 0.1247 - val_mse: 0.0315\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0193 - mae: 0.0898 - mse: 0.0193 - val_loss: 0.0318 - val_mae: 0.1253 - val_mse: 0.0318\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0191 - mae: 0.0893 - mse: 0.0191 - val_loss: 0.0309 - val_mae: 0.1235 - val_mse: 0.0309\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0189 - mae: 0.0891 - mse: 0.0189 - val_loss: 0.0316 - val_mae: 0.1251 - val_mse: 0.0316\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0191 - mae: 0.0885 - mse: 0.0191 - val_loss: 0.0313 - val_mae: 0.1245 - val_mse: 0.0313\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0189 - mae: 0.0891 - mse: 0.0189 - val_loss: 0.0307 - val_mae: 0.1229 - val_mse: 0.0307\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0185 - mae: 0.0873 - mse: 0.0185 - val_loss: 0.0313 - val_mae: 0.1245 - val_mse: 0.0313\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0186 - mae: 0.0874 - mse: 0.0186 - val_loss: 0.0311 - val_mae: 0.1237 - val_mse: 0.0311\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0184 - mae: 0.0870 - mse: 0.0184 - val_loss: 0.0311 - val_mae: 0.1238 - val_mse: 0.0311\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0182 - mae: 0.0865 - mse: 0.0182 - val_loss: 0.0307 - val_mae: 0.1225 - val_mse: 0.0307\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0181 - mae: 0.0872 - mse: 0.0181 - val_loss: 0.0314 - val_mae: 0.1242 - val_mse: 0.0314\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0180 - mae: 0.0853 - mse: 0.0180 - val_loss: 0.0310 - val_mae: 0.1234 - val_mse: 0.0310\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0179 - mae: 0.0865 - mse: 0.0179 - val_loss: 0.0311 - val_mae: 0.1231 - val_mse: 0.0311\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0177 - mae: 0.0842 - mse: 0.0177 - val_loss: 0.0314 - val_mae: 0.1237 - val_mse: 0.0314\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0175 - mae: 0.0860 - mse: 0.0175 - val_loss: 0.0313 - val_mae: 0.1230 - val_mse: 0.0313\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0174 - mae: 0.0838 - mse: 0.0174 - val_loss: 0.0313 - val_mae: 0.1231 - val_mse: 0.0313\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0172 - mae: 0.0847 - mse: 0.0172 - val_loss: 0.0315 - val_mae: 0.1234 - val_mse: 0.0315\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0170 - mae: 0.0837 - mse: 0.0170 - val_loss: 0.0317 - val_mae: 0.1239 - val_mse: 0.0317\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0170 - mae: 0.0832 - mse: 0.0170 - val_loss: 0.0310 - val_mae: 0.1225 - val_mse: 0.0310\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0167 - mae: 0.0825 - mse: 0.0167 - val_loss: 0.0313 - val_mae: 0.1226 - val_mse: 0.0313\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0166 - mae: 0.0830 - mse: 0.0166 - val_loss: 0.0316 - val_mae: 0.1230 - val_mse: 0.0316\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0165 - mae: 0.0821 - mse: 0.0165 - val_loss: 0.0314 - val_mae: 0.1227 - val_mse: 0.0314\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0164 - mae: 0.0823 - mse: 0.0164 - val_loss: 0.0314 - val_mae: 0.1227 - val_mse: 0.0314\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0165 - mae: 0.0824 - mse: 0.0165 - val_loss: 0.0310 - val_mae: 0.1209 - val_mse: 0.0310\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0161 - mae: 0.0814 - mse: 0.0161 - val_loss: 0.0322 - val_mae: 0.1242 - val_mse: 0.0322\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0162 - mae: 0.0814 - mse: 0.0162 - val_loss: 0.0314 - val_mae: 0.1223 - val_mse: 0.0314\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0160 - mae: 0.0813 - mse: 0.0160 - val_loss: 0.0319 - val_mae: 0.1234 - val_mse: 0.0319\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0159 - mae: 0.0804 - mse: 0.0159 - val_loss: 0.0310 - val_mae: 0.1209 - val_mse: 0.0310\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0159 - mae: 0.0815 - mse: 0.0159 - val_loss: 0.0320 - val_mae: 0.1232 - val_mse: 0.0320\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0157 - mae: 0.0795 - mse: 0.0157 - val_loss: 0.0312 - val_mae: 0.1211 - val_mse: 0.0312\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0157 - mae: 0.0811 - mse: 0.0157 - val_loss: 0.0315 - val_mae: 0.1216 - val_mse: 0.0315\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0156 - mae: 0.0802 - mse: 0.0156 - val_loss: 0.0321 - val_mae: 0.1233 - val_mse: 0.0321\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0158 - mae: 0.0793 - mse: 0.0158 - val_loss: 0.0312 - val_mae: 0.1205 - val_mse: 0.0312\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0156 - mae: 0.0824 - mse: 0.0156 - val_loss: 0.0314 - val_mae: 0.1213 - val_mse: 0.0314\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0157 - mae: 0.0792 - mse: 0.0157 - val_loss: 0.0310 - val_mae: 0.1201 - val_mse: 0.0310\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0155 - mae: 0.0808 - mse: 0.0155 - val_loss: 0.0316 - val_mae: 0.1211 - val_mse: 0.0316\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0153 - mae: 0.0783 - mse: 0.0153 - val_loss: 0.0318 - val_mae: 0.1215 - val_mse: 0.0318\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0155 - mae: 0.0811 - mse: 0.0155 - val_loss: 0.0325 - val_mae: 0.1236 - val_mse: 0.0325\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0151 - mae: 0.0781 - mse: 0.0151 - val_loss: 0.0313 - val_mae: 0.1205 - val_mse: 0.0313\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0150 - mae: 0.0783 - mse: 0.0150 - val_loss: 0.0322 - val_mae: 0.1221 - val_mse: 0.0322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27f20f43400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_scaled, y, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nn_scaler.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "model.save('nn_model.h5')  \n",
    "joblib.dump(scaler, 'nn_scaler.joblib')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "def best_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first units',\n",
    "                                                    min_value = 1,\n",
    "                                                    max_value = 11,\n",
    "                                                    step = 2), activation = activation, input_dim = 11))\n",
    "\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 6)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                                    min_value = 1,\n",
    "                                                    max_value = 22,\n",
    "                                                    step = 2), activation = activation))\n",
    "    nn_model.add(tf.keras.layers.Dense(units = 1, activation = 'linear'))\n",
    "\n",
    "\n",
    "    nn_model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mse', 'mae'])\n",
    "\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "#Kerastuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    best_model,\n",
    "    objective = 'mse',\n",
    "    max_epochs = 20,\n",
    "    hyperband_iterations= 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 72 Complete [00h 00m 12s]\n",
      "mse: 0.05969971418380737\n",
      "\n",
      "Best mse So Far: 0.018799250945448875\n",
      "Total elapsed time: 00h 01m 56s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_scaled, y, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'first units': 9,\n",
       " 'num_layers': 2,\n",
       " 'units_0': 15,\n",
       " 'units_1': 17,\n",
       " 'units_2': 17,\n",
       " 'units_3': 15,\n",
       " 'units_4': 7,\n",
       " 'units_5': 21,\n",
       " 'tuner/epochs': 20,\n",
       " 'tuner/initial_epoch': 7,\n",
       " 'tuner/bracket': 2,\n",
       " 'tuner/round': 2,\n",
       " 'tuner/trial_id': '0012'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 9)                 108       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 15)                150       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 17)                272       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 548 (2.14 KB)\n",
      "Trainable params: 548 (2.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 4s 44ms/step - loss: 0.1412 - mse: 0.1412 - mae: 0.2766 - val_loss: 0.1205 - val_mse: 0.1205 - val_mae: 0.2366\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0865 - mse: 0.0865 - mae: 0.2086 - val_loss: 0.0870 - val_mse: 0.0870 - val_mae: 0.2078\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0642 - mse: 0.0642 - mae: 0.1775 - val_loss: 0.0717 - val_mse: 0.0717 - val_mae: 0.1977\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0546 - mse: 0.0546 - mae: 0.1646 - val_loss: 0.0638 - val_mse: 0.0638 - val_mae: 0.1929\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0498 - mse: 0.0498 - mae: 0.1602 - val_loss: 0.0557 - val_mse: 0.0557 - val_mae: 0.1905\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0453 - mse: 0.0453 - mae: 0.1523 - val_loss: 0.0494 - val_mse: 0.0494 - val_mae: 0.1810\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0414 - mse: 0.0414 - mae: 0.1436 - val_loss: 0.0453 - val_mse: 0.0453 - val_mae: 0.1727\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0380 - mse: 0.0380 - mae: 0.1394 - val_loss: 0.0415 - val_mse: 0.0415 - val_mae: 0.1671\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0353 - mse: 0.0353 - mae: 0.1339 - val_loss: 0.0385 - val_mse: 0.0385 - val_mae: 0.1601\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0331 - mse: 0.0331 - mae: 0.1292 - val_loss: 0.0363 - val_mse: 0.0363 - val_mae: 0.1547\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0312 - mse: 0.0312 - mae: 0.1238 - val_loss: 0.0347 - val_mse: 0.0347 - val_mae: 0.1498\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0293 - mse: 0.0293 - mae: 0.1195 - val_loss: 0.0334 - val_mse: 0.0334 - val_mae: 0.1457\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0279 - mse: 0.0279 - mae: 0.1164 - val_loss: 0.0325 - val_mse: 0.0325 - val_mae: 0.1432\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1129 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1382\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0257 - mse: 0.0257 - mae: 0.1092 - val_loss: 0.0305 - val_mse: 0.0305 - val_mae: 0.1361\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0254 - mse: 0.0254 - mae: 0.1059 - val_loss: 0.0298 - val_mse: 0.0298 - val_mae: 0.1334\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0245 - mse: 0.0245 - mae: 0.1063 - val_loss: 0.0291 - val_mse: 0.0291 - val_mae: 0.1314\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1021 - val_loss: 0.0289 - val_mse: 0.0289 - val_mae: 0.1303\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.0998 - val_loss: 0.0285 - val_mse: 0.0285 - val_mae: 0.1288\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.0998 - val_loss: 0.0282 - val_mse: 0.0282 - val_mae: 0.1285\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0224 - mse: 0.0224 - mae: 0.0982 - val_loss: 0.0278 - val_mse: 0.0278 - val_mae: 0.1267\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0220 - mse: 0.0220 - mae: 0.0971 - val_loss: 0.0276 - val_mse: 0.0276 - val_mae: 0.1264\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0216 - mse: 0.0216 - mae: 0.0956 - val_loss: 0.0272 - val_mse: 0.0272 - val_mae: 0.1250\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.0956 - val_loss: 0.0273 - val_mse: 0.0273 - val_mae: 0.1249\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.0939 - val_loss: 0.0272 - val_mse: 0.0272 - val_mae: 0.1246\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.0934 - val_loss: 0.0271 - val_mse: 0.0271 - val_mae: 0.1243\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.0919 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1240\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.0911 - val_loss: 0.0267 - val_mse: 0.0267 - val_mae: 0.1225\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0917 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1222\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0905 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1239\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0895 - val_loss: 0.0267 - val_mse: 0.0267 - val_mae: 0.1229\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0882 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1246\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0880 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1227\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0884 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1229\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0182 - mse: 0.0182 - mae: 0.0867 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1240\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0180 - mse: 0.0180 - mae: 0.0851 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1224\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0178 - mse: 0.0178 - mae: 0.0856 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1224\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0175 - mse: 0.0175 - mae: 0.0848 - val_loss: 0.0268 - val_mse: 0.0268 - val_mae: 0.1241\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0174 - mse: 0.0174 - mae: 0.0849 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1214\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0173 - mse: 0.0173 - mae: 0.0837 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1227\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0170 - mse: 0.0170 - mae: 0.0828 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1219\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0169 - mse: 0.0169 - mae: 0.0835 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1228\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0167 - mse: 0.0167 - mae: 0.0825 - val_loss: 0.0263 - val_mse: 0.0263 - val_mae: 0.1207\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0166 - mse: 0.0166 - mae: 0.0815 - val_loss: 0.0266 - val_mse: 0.0266 - val_mae: 0.1226\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0164 - mse: 0.0164 - mae: 0.0821 - val_loss: 0.0262 - val_mse: 0.0262 - val_mae: 0.1199\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0164 - mse: 0.0164 - mae: 0.0812 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1218\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0161 - mse: 0.0161 - mae: 0.0801 - val_loss: 0.0263 - val_mse: 0.0263 - val_mae: 0.1196\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0160 - mse: 0.0160 - mae: 0.0791 - val_loss: 0.0267 - val_mse: 0.0267 - val_mae: 0.1218\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0160 - mse: 0.0160 - mae: 0.0818 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1209\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0158 - mse: 0.0158 - mae: 0.0799 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1200\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0157 - mse: 0.0157 - mae: 0.0788 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1197\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0155 - mse: 0.0155 - mae: 0.0788 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1201\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0155 - mse: 0.0155 - mae: 0.0783 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1197\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0156 - mse: 0.0156 - mae: 0.0803 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1192\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0152 - mse: 0.0152 - mae: 0.0779 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1191\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0151 - mse: 0.0151 - mae: 0.0770 - val_loss: 0.0262 - val_mse: 0.0262 - val_mae: 0.1181\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0788 - val_loss: 0.0263 - val_mse: 0.0263 - val_mae: 0.1178\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0150 - mse: 0.0150 - mae: 0.0763 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1182\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0150 - mse: 0.0150 - mae: 0.0782 - val_loss: 0.0266 - val_mse: 0.0266 - val_mae: 0.1189\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0773 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1185\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0764 - val_loss: 0.0266 - val_mse: 0.0266 - val_mae: 0.1188\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0762 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1180\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0762 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1176\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0761 - val_loss: 0.0267 - val_mse: 0.0267 - val_mae: 0.1199\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0792 - val_loss: 0.0263 - val_mse: 0.0263 - val_mae: 0.1173\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0763 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1174\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0748 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1183\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0147 - mse: 0.0147 - mae: 0.0773 - val_loss: 0.0262 - val_mse: 0.0262 - val_mae: 0.1157\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0769 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1206\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0759 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1167\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0737 - val_loss: 0.0267 - val_mse: 0.0267 - val_mae: 0.1183\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0141 - mse: 0.0141 - mae: 0.0751 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1165\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0743 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1167\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0139 - mse: 0.0139 - mae: 0.0733 - val_loss: 0.0266 - val_mse: 0.0266 - val_mae: 0.1170\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0139 - mse: 0.0139 - mae: 0.0741 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1157\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0138 - mse: 0.0138 - mae: 0.0734 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1162\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0730 - val_loss: 0.0264 - val_mse: 0.0264 - val_mae: 0.1161\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0735 - val_loss: 0.0267 - val_mse: 0.0267 - val_mae: 0.1162\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0729 - val_loss: 0.0268 - val_mse: 0.0268 - val_mae: 0.1171\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0726 - val_loss: 0.0265 - val_mse: 0.0265 - val_mae: 0.1153\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0728 - val_loss: 0.0266 - val_mse: 0.0266 - val_mae: 0.1174\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0737 - val_loss: 0.0271 - val_mse: 0.0271 - val_mae: 0.1173\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0134 - mse: 0.0134 - mae: 0.0732 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1175\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0718 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1182\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0720 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1173\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0725 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1181\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0725 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1164\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0719 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1166\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0717 - val_loss: 0.0273 - val_mse: 0.0273 - val_mae: 0.1179\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0708 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1166\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0726 - val_loss: 0.0274 - val_mse: 0.0274 - val_mae: 0.1170\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0714 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1170\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0703 - val_loss: 0.0275 - val_mse: 0.0275 - val_mae: 0.1186\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0712 - val_loss: 0.0274 - val_mse: 0.0274 - val_mae: 0.1167\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0701 - val_loss: 0.0271 - val_mse: 0.0271 - val_mae: 0.1175\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0706 - val_loss: 0.0276 - val_mse: 0.0276 - val_mae: 0.1180\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0699 - val_loss: 0.0278 - val_mse: 0.0278 - val_mae: 0.1201\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0709 - val_loss: 0.0274 - val_mse: 0.0274 - val_mae: 0.1168\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0705 - val_loss: 0.0282 - val_mse: 0.0282 - val_mae: 0.1230\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0712 - val_loss: 0.0276 - val_mse: 0.0276 - val_mae: 0.1181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27f2908fe20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model = tf.keras.models.Sequential()\n",
    "tuned_model.add(tf.keras.layers.Dense(units=9, activation='relu', input_dim = 11))\n",
    "tuned_model.add(tf.keras.layers.Dense(units=15, activation='relu'))\n",
    "tuned_model.add(tf.keras.layers.Dense(units=17, activation='relu'))\n",
    "tuned_model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "\n",
    "tuned_model.summary()\n",
    "\n",
    "tuned_model.compile(loss='mean_squared_error', optimizer = 'adam', metrics = ['mse', 'mae'])\n",
    "\n",
    "tuned_model.fit(X_scaled, y, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "tuned_model.save('tuned_nn_model.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Model \n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "lr_X = mvp_voting_df.copy()\n",
    "lr_X.drop(columns=['Unnamed: 0', 'player', 'season', 'votes_first', 'points_won', 'points_max', 'award_share'], axis=1, inplace=True)\n",
    "lr_y = mvp_voting_df['award_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_names</th>\n",
       "      <th>feature_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ws</td>\n",
       "      <td>417.928709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ws_per_48</td>\n",
       "      <td>354.117235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>per</td>\n",
       "      <td>347.542067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bpm</td>\n",
       "      <td>317.003508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pts_per_g</td>\n",
       "      <td>144.043844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fta</td>\n",
       "      <td>93.138309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>win_pct</td>\n",
       "      <td>92.627672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>usg_pct</td>\n",
       "      <td>91.893246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fga</td>\n",
       "      <td>73.265688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ts_pct</td>\n",
       "      <td>40.267982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mp_per_g</td>\n",
       "      <td>38.561019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>trb_per_g</td>\n",
       "      <td>22.193255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fg_pct</td>\n",
       "      <td>14.543791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>blk_per_g</td>\n",
       "      <td>12.635152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stl_per_g</td>\n",
       "      <td>11.733180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ast_per_g</td>\n",
       "      <td>10.564230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g</td>\n",
       "      <td>10.046205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fg3a</td>\n",
       "      <td>8.904435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ft_pct</td>\n",
       "      <td>2.283431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fg3_pct</td>\n",
       "      <td>0.612034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_names  feature_scores\n",
       "18            ws      417.928709\n",
       "19     ws_per_48      354.117235\n",
       "3            per      347.542067\n",
       "6            bpm      317.003508\n",
       "10     pts_per_g      144.043844\n",
       "2            fta       93.138309\n",
       "7        win_pct       92.627672\n",
       "5        usg_pct       91.893246\n",
       "0            fga       73.265688\n",
       "4         ts_pct       40.267982\n",
       "9       mp_per_g       38.561019\n",
       "11     trb_per_g       22.193255\n",
       "15        fg_pct       14.543791\n",
       "14     blk_per_g       12.635152\n",
       "13     stl_per_g       11.733180\n",
       "12     ast_per_g       10.564230\n",
       "8              g       10.046205\n",
       "1           fg3a        8.904435\n",
       "17        ft_pct        2.283431\n",
       "16       fg3_pct        0.612034"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear Model Feature Selection \n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "fs = SelectKBest(score_func=f_regression, k='all')\n",
    "fs.fit(lr_X,lr_y)\n",
    "lr_X_fs = fs.transform(lr_X)\n",
    "feature_names = fs.get_feature_names_out()\n",
    "fs_df = pd.DataFrame({'feature_names': feature_names, 'feature_scores': fs.scores_})\n",
    "fs_df.sort_values(by = 'feature_scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_scaler.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear Model w/ top 7 features\n",
    "updated_lr_X = lr_X[['ws', 'ws_per_48','per', 'bpm', 'pts_per_g', 'win_pct', 'usg_pct']]\n",
    "lr_scaler = StandardScaler()\n",
    "updated_lr_X_scaled = lr_scaler.fit_transform(updated_lr_X)\n",
    "lr_model.fit(updated_lr_X_scaled, lr_y)\n",
    "joblib.dump(lr_model, 'lr_model.joblib')  \n",
    "joblib.dump(lr_scaler, 'lr_scaler.joblib')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_X = mvp_voting_df.copy()\n",
    "rf_X.drop(columns=['Unnamed: 0', 'player', 'season', 'votes_first', 'points_won', 'points_max', 'award_share'], axis=1, inplace=True)\n",
    "rf_y = mvp_voting_df['award_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3917624990923945, 'ws'),\n",
       " (0.1273102273962831, 'win_pct'),\n",
       " (0.09112543451046652, 'per'),\n",
       " (0.05778553826528482, 'bpm'),\n",
       " (0.043540489517779744, 'ws_per_48'),\n",
       " (0.03535663053947361, 'usg_pct'),\n",
       " (0.03348384340760627, 'fga'),\n",
       " (0.02527649055226881, 'ft_pct'),\n",
       " (0.022963301010964755, 'ast_per_g'),\n",
       " (0.022407436306942752, 'mp_per_g'),\n",
       " (0.02118451376981331, 'fta'),\n",
       " (0.021113093425223765, 'fg_pct'),\n",
       " (0.01763241842930172, 'pts_per_g'),\n",
       " (0.015601249034509917, 'ts_pct'),\n",
       " (0.015314203172930849, 'trb_per_g'),\n",
       " (0.01438073761509134, 'fg3_pct'),\n",
       " (0.013528987256828585, 'g'),\n",
       " (0.010963354597181086, 'blk_per_g'),\n",
       " (0.009913322547373418, 'fg3a'),\n",
       " (0.009356229552281103, 'stl_per_g')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_scaler = StandardScaler()\n",
    "rf_X_scaled = rf_scaler.fit_transform(rf_X)\n",
    "rf_model = RandomForestRegressor(n_estimators=500, random_state=78)\n",
    "rf_model.fit(rf_X_scaled, rf_y)\n",
    "feature_importance = rf_model.feature_importances_\n",
    "sorted(zip(rf_model.feature_importances_, rf_X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_scaler.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Updated model using feature importance with 6 features from nn_model\n",
    "updated_rf_X = rf_X[['per', 'bpm', 'usg_pct', 'win_pct', 'ws', 'ws_per_48']]\n",
    "updated_rf_X_scaled = rf_scaler.fit_transform(updated_rf_X)\n",
    "updated_rf_model = RandomForestRegressor(n_estimators=500, random_state=78)\n",
    "updated_rf_model.fit(updated_rf_X_scaled, rf_y)\n",
    "joblib.dump(updated_rf_model, 'rf_model.joblib')\n",
    "joblib.dump(rf_scaler, 'rf_scaler.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
